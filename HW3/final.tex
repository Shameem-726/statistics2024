\documentclass[10pt,letterpaper]{article}
\usepackage[margin=.75in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{pgfplots}
\usepackage[shortlabels]{enumitem}
\usepackage{listings}
\usepackage[document]{ragged2e}
\usepackage{setspace}
\usepackage{textcomp}
\usepackage[thinc]{esdiff}

\newcommand{\mytexttilde}{\raisebox{0.5ex}{\texttildelow}}

\onehalfspacing

\author{Orkun Akyol}
\title{Statistics For Data Science: HW 3}

\pagestyle{fancy}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
\lstset{frame=tb,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}
\setlength{\headheight}{25pt}
\fancyhf{}
\rhead{
    Statistics for DS Winter 2024 | Exercise 3    
}
\rfoot{\thepage}

\begin{document}

\paragraph{1.}
Per given hint, we will calculate the expression $P(X^2<t)$, which is the CDF of the random variable Y. Differentiating should then give us the PDF. 

    \begin{equation}
        F_Y = P(X^2 < t) = P(-\sqrt{t} < X < \sqrt{t})
    \end{equation}
Since X is a standard normal variable, and the normal distribution is symmetric, we get:  
    \begin{equation}
        F_Y = \Phi(\sqrt{t}) - \Phi(-\sqrt{t}) = 2\Phi(\sqrt{t}) - 1
    \end{equation}
Now that we got the CDF of Y, we have to differentiate according to t. Via the chain rule: 

    \begin{equation}
       \diff{(2\Phi(\sqrt{t})-1)}{t} = 2(\frac{1}{2\sqrt{t}})\varphi(\sqrt{t})
    \end{equation}

Simplifying and plugging in the PDF of normal distribution results in the following: 
    \begin{equation}
       f_Y(t) = \frac{1}{\sqrt{2\pi t}}e^{-t/2}
    \end{equation}
This is obviously only relevant for $t>0$. For $t<0$, $f_Y(t) = 0$ since $X^2$ is never $0$.
\paragraph{2.}
Expected Value:
\[
E(|X|) = \int_{-\infty}^{\infty} |x| \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{1}{2 \sigma^2} (|x| - \mu)^2} \, dx
\]

Given that \( \mu = 0 \), \( \sigma^2 = 1 \) and that \( |X| \) is symmetric on \( 0 \), it becomes:

\[
E(|X|) = 2 \cdot \int_{0}^{\infty} x \cdot \frac{1}{\sqrt{2 \pi}} e^{-\frac{x^2}{2}} \, dx
\]

\[
= \frac{2}{\sqrt{2 \pi}} \int_{0}^{\infty} x e^{-\frac{x^2}{2}} \, dx
\]

Focusing on the integral, we can substitute \(x\) with \( u = \frac{x^2}{2} \), resulting in \(du = x\, dx\) and therefore:

\[
= \int_{0}^{\infty} \frac{du}{dx} e^{-u} \, dx = \int_{0}^{\infty} e^{-u} \, du = 1
\]

which leads to:

\[
E(|X|) = \frac{2}{\sqrt{2 \pi}} \int_{0}^{\infty} e^{-u} \, du = \frac{2}{\sqrt{2 \pi}} = \sqrt{\frac{2}{\pi}} 
\]  
Variance:
\[
Var(|X|) = E(|X|^2) - \mu^2
\]
Noting that $X\mytexttilde N(0,1)$ is centered: 
\[
E(X^2)=Var(X)=\sigma^2=1
\]
Noting that \(|X|^2=X^2\), \(E(|X|^2)=E(X^2)=1\) holds.
From the previous result we can compute \(\mu^2\):
\[
\mu^2 = (E(|X|))^2 =(\sqrt{\frac{2}{\pi}})^2
\]
Finally:
\[
Var(|X|)=1-\frac{2}{\pi}
\]

\paragraph{4.}

    \begin{equation}
        \mathbb{E}[\left \Vert X-\mu \right \Vert ^2] = 
        \mathbb{E}[(X-\mu)^T(X-\mu)]
    \end{equation}
    
    \begin{equation}
       \mathbb{E}[\left \Vert X-\mu \right \Vert ^2] = 
       \mathbb{E}[X^TX - 2X^T\mu + \mu^T\mu]
    \end{equation}

    \begin{equation}
       \mathbb{E}[\left \Vert X-\mu \right \Vert ^2] = 
       \mathbb{E}[X^TX] - \mu^T\mu
    \end{equation}

Now let us look at the term $\mathbb{E}[X^TX]$. It is equivalent to $\mathbb{E}[x_1^2 + x_2^2 + ... + x_d^2]$. We know that expected value of the sum of variables is equivalent to the sum of expected values of individual variables: 

    \begin{equation}
       \mathbb{E}[x_1^2 + x_2^2 + ... + x_d^2] = 
       \mathbb{E}[x_1^2] + \mathbb{E}[x_2^2] + ... + \mathbb{E}[x_d^2]
    \end{equation}

On all of the terms we can apply the theorem of Variance Translation: 

    \begin{equation}
       Var(X) =  \mathbb{E}[X^2] - \mu^2
    \end{equation}

    \begin{equation}
       \mathbb{E}[x_1^2] + \mathbb{E}[x_2^2] + ... + \mathbb{E}[x_d^2] = (Var(x_1) + \mu_1^2) + (Var(x_2) + \mu_2^2) + ... + (Var(x_d) + \mu_d^2)
    \end{equation}

We know that the diagonal values on the covariance matrix are variances for individual variables. The sum of them is equal to the trace of the matrix. If we also write the sum of individual $\mu$ values in matrix form, we get: 

    \begin{equation}
       \mathbb{E}[X^TX] = tr(C) + \mu^T\mu
    \end{equation}

Substituting this into (3), we get:

    \begin{equation}
       \mathbb{E}[\left \Vert X-\mu \right \Vert ^2] = 
       tr(C) + \mu^T\mu- \mu^T\mu = tr(C)
    \end{equation}

This completes the proof. 
\end{document}
